{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_pipline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anushajadav/Data-pipeline-using-python/blob/main/data_pipline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4S8YOpSY-Sp"
      },
      "source": [
        "import numpy as nm  \n",
        "\n",
        "import matplotlib.pyplot as mtp  \n",
        "\n",
        "import pandas as pd  \n",
        "\n",
        "# 1 Take a pandas dataframe\n",
        "dataset=pd.read_csv('dataset.csv')\n",
        "preprocessed_dataset=data_pipline(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7n1K4Sx7Oyf"
      },
      "source": [
        "def missing_values(dataset):\n",
        "    features_nan=[feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes=='O']\n",
        "    #replace the categorical values with label Missing\n",
        "    def replace_cat_feature(dataset,features_nan):\n",
        "        data=dataset.copy()\n",
        "        data[features_nan]=data[features_nan].fillna('Missing')\n",
        "        return data\n",
        "\n",
        "    dataset=replace_cat_feature(dataset,features_nan)\n",
        "    #missing of numerical\n",
        "    numerical_with_nan=[feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes!='O']\n",
        "    for feature in numerical_with_nan:\n",
        "        ## We will replace by using median since there are outliers\n",
        "        median_value=dataset[feature].median()\n",
        "        dataset[feature].fillna(median_value,inplace=True)\n",
        "    categorical_features=[feature for feature in dataset.columns if dataset[feature].dtype=='O']\n",
        "    #rare values- \n",
        "    for feature in categorical_features:\n",
        "        temp=dataset.groupby(feature)['target'].count()/len(dataset)\n",
        "        temp_df=temp[temp>0.01].index\n",
        "        dataset[feature]=np.where(dataset[feature].isin(temp_df),dataset[feature],'Rare_var')\n",
        "    return dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CW09K0WZAE6"
      },
      "source": [
        "#Binary classifcation\n",
        "def over_under_sampling(data):\n",
        "     \n",
        "    class_count_0, class_count_1 = data['target'].value_counts()\n",
        "    print('Before sampling')\n",
        "    print(data['target'].value_counts())\n",
        "    \n",
        "    # check the ration\n",
        "    if class_count_0>class_count_1:\n",
        "        val=class_count_0/class_count_1\n",
        "    else:\n",
        "        val=class_count_1/class_count_0\n",
        "        \n",
        "    #checking the data is balanced or not\n",
        "    if val<1.5: # less than 1.5 data is balanced\n",
        "        print(\"The data is balanced\")\n",
        "        pass\n",
        "    else:\n",
        "        print(\"data is imbalaced, Sampling required\")\n",
        "        class_count_0, class_count_1 = data['target'].value_counts()\n",
        "        class_0 = data[data['target'] == 0]\n",
        "        class_1 = data[data['target'] == 1]\n",
        "        #checking undersampling or oversampling\n",
        "        # we have assumed million is large dataset\n",
        "        if data.shape[0] > 1000000:\n",
        "            #if large data -Under sampling.\n",
        "            print(\"Under sampling required, because the size of data is more than million \")\n",
        "            if class_count_0>class_count_1:\n",
        "                print(\" No class belongs1 0 - {} > No class belongs 1 - {}\".format(class_count_0 ,class_count_1 ))\n",
        "                print(\"So we have to take some sample from class 0 with the size equal to class 1 \")\n",
        "                class_0_under = class_0.sample(class_count_1)\n",
        "                test_under = pd.concat([class_0_under, class_1], axis=0)\n",
        "                print(\"After sampling\")\n",
        "                print(test_under['target'].value_counts())\n",
        "                return test_under\n",
        "            else:\n",
        "                print(\" No class belongs1 1 - {} > No class belongs 0 - {}\".format(class_count_1 ,class_count_0 ))\n",
        "                print(\"So we have to take some sample from class 1 with the size equal to class 0 \")\n",
        "                class_1_under = class_1.sample(class_count_0)\n",
        "                test_under = pd.concat([class_0_under, class_0], axis=0)\n",
        "                print(\"After sampling\")\n",
        "                print(test_under['target'].value_counts())\n",
        "                return test_under\n",
        "\n",
        "        else:\n",
        "            #for  small-medium data we do -over sampling.\n",
        "            print(\"over sampling required\")\n",
        "            if class_count_0>class_count_1:\n",
        "                print(\" No class belongs1 0 - {} > No class belongs 1 - {}\".format(class_count_0 ,class_count_1 ))\n",
        "                print(\"So we have to add more sample to class 1\")\n",
        "                class_oversampling_1 = class_1.sample(class_count_0, replace=True)\n",
        "                data_oversampling = pd.concat([class_oversampling_1, class_0], axis=0)\n",
        "                print(\"After sampling\")\n",
        "                print(data_oversampling['target'].value_counts())\n",
        "                return data_oversampling\n",
        "            else:\n",
        "                print(\" No class belongs 1 - {} > No class belongs 0 - {}\".format(class_count_1 ,class_count_0 ))\n",
        "                print(\"So we have to add more sample to class 0\")\n",
        "                class_oversampling_0 = class_0.sample(class_count_1, replace=True)\n",
        "                data_oversampling = pd.concat([class_oversampling_0, class_0], axis=0)\n",
        "                print(\"After sampling\")\n",
        "                print(data_oversampling['target'].value_counts())\n",
        "                return data_oversampling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBkMJWkxeXKP"
      },
      "source": [
        "def correlation_matrix(dataset):\n",
        "    import seaborn as sns\n",
        "    #Using Pearson Correlation\n",
        "    plt.figure(figsize=(20,20))\n",
        "    cor = dataset.corr()\n",
        "    sns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\n",
        "    plt.show()\n",
        "\n",
        "    def correlation(dataset, threshold):\n",
        "        col_corr = set()  # Set of all the names of correlated columns\n",
        "        corr_matrix = dataset.corr()\n",
        "        for i in range(len(corr_matrix.columns)):\n",
        "            for j in range(i):\n",
        "                if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
        "                    colname = corr_matrix.columns[i]  # getting the name of column\n",
        "                    col_corr.add(colname)\n",
        "        return col_corr\n",
        "\n",
        "    columns_correlated=correlation(dataset,90)\n",
        "    print(\"highly correlated columns\",columns_correlated)\n",
        "    dataset.drop(columns_correlated,axis=1)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7X6znSTDBcr"
      },
      "source": [
        "#Inferential Statistics - wl take some data for testing ,based on efficiency will apply to rest data.\n",
        "#   example - Vaccine. First we take small sample  from population for checking vaccine effieciency and then infer the vaccine for whole population\n",
        "#descriptive Statistics- We conduct on entire population. \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEWFu7WNrrGR"
      },
      "source": [
        "How can we remove outlier.\n",
        "USING Percentile:\n",
        "50 percentile-means 50% data points are less than 50% percentile.\n",
        "100 percentile: for which data point the values are less than or equal to that value.\n",
        "How to find the percentile?\n",
        "\n",
        "say 25 percntile.\n",
        "1) Find the lenght of column.\n",
        "2) 25% of (length of column)\n",
        "\n",
        "Mode: most frequency used.\n",
        "\n",
        "IQR.\n",
        "25th percentile .\n",
        "lets consider we hv 20 samples.\n",
        "25% of 20 is 25/100 *20 =5  q1\n",
        "we shd find the value bw 5th and 6th element.\n",
        " \n",
        "75% of 20  = 75/100*20= 15   q3\n",
        "we shd find the record bw 15 and 16th row.\n",
        "\n",
        "25TH percentile Q1\n",
        "50TH Q2\n",
        "75TH Q3\n",
        "\n",
        "\n",
        "IQR= Q3-Q1.\n",
        "\n",
        "to remove outliers...we hv to come with lower limit and upper limit.\n",
        "\n",
        "lower limit= Q1-1.5*IQR\n",
        "upper limit=Q3-1.5*IQR\n",
        "\n",
        "WE can exclude the lower_limit and upper_limit.\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrOQx8tHragu"
      },
      "source": [
        "def descriptive_statics(dataset):\n",
        "      report={}\n",
        "      numerical_feature =[feature for feature in dataset.columns if dataset[feature].dtypes!='O']\n",
        "      \n",
        "      report_des={feature:[] for feature in numerical_feature}\n",
        "      datasets=dataset.copy()\n",
        "      datasets.fillna(0)\n",
        "      for feature in numerical_feature:\n",
        "        \n",
        "            #count\n",
        "            count=len(datasets[feature])\n",
        "            report_des[feature].append(count)\n",
        "            \n",
        "            #mean\n",
        "            mean= mean=np.round(sum(datasets[feature])/count,6)\n",
        "            report_des[feature].append(mean)\n",
        "\n",
        "            #find the median\n",
        "             \n",
        "            sorted_data=sorted(datasets[feature]) \n",
        "            median=0\n",
        "            if count%2==0:\n",
        "                 median_=(sorted_data[int(count/2)]+sorted_data[int(count/2)-1])/2\n",
        "           \n",
        "            else:\n",
        "                 median_=sorted_data[count/2]\n",
        "            report_des[feature].append(median_)        \n",
        "             \n",
        "\n",
        "            #mode.\n",
        "            mode_= dict(dataset[feature].value_counts()) \n",
        "            mode_ = sorted(mode_.items(), key=lambda x: x[1], reverse=True)\n",
        "            mode_[0][0]\n",
        "            report_des[feature].append(mode_[0][0])\n",
        "\n",
        "            #min\n",
        "            min_=sorted(datasets[feature])[0]\n",
        "            report_des[feature].append(min_)\n",
        "\n",
        "            #max\n",
        "            max_=sorted(datasets[feature],reverse=True)[0]\n",
        "            report_des[feature].append(max_)\n",
        "            \n",
        "            #25%            \n",
        "\n",
        "            index=0.25*count\n",
        "             \n",
        "            if index%10==0:   \n",
        "                index=int(index)\n",
        "                percentile_25th=sorted_data[index]\n",
        "            else:\n",
        "                index=int(index)\n",
        "                percentile_25th=(sorted_data[index]+sorted_data[index+1])/2\n",
        "            report_des[feature].append(percentile_25th)\n",
        "\n",
        "            #75%\n",
        "            index=0.75*count\n",
        "             \n",
        "            if index%10==0:\n",
        "                index=int(index)\n",
        "                percentile_75th=sorted_data[index]\n",
        "            else:\n",
        "                index=int(index)\n",
        "                percentile_75th=(sorted_data[index]+sorted_data[index+1])/2\n",
        "\n",
        "            report_des[feature].append(percentile_75th) \n",
        "\n",
        "            #IQR\n",
        "            IQR=percentile_75th-percentile_25th\n",
        "            report_des[feature].append(IQR) \n",
        "      reports=pd.DataFrame(report_des)\n",
        "      reports.index=['count','mean','medean','mode','min','max','25%','75%','IQR']\n",
        "      return reports"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gP49LPVivyE"
      },
      "source": [
        "#assuming the dependent variable as target and primary key as id. assuming for numerical values has outlier.\n",
        "def data_pipine(data):\n",
        "    data.columns = [*data.columns[:-1], 'target']\n",
        "    #2 prints the null values present in each column\n",
        "    features_has_na=[features for features in data.columns if data[features].isnull().sum()>1]\n",
        "    for feature in features_has_na:\n",
        "        print(feature, np.round(data[feature].isnull().mean(), 4),  ' % missing values')\n",
        "\n",
        "    #2 Missing value handeling\n",
        "    data=missing_values(data)\n",
        "    \n",
        "    #4 categorical transformation to int.\n",
        "    categorical_features=[feature for feature in data.columns if dataset[feature].dtype=='O']\n",
        "    for feature in categorical_features:\n",
        "        labels_ordered=data.groupby([feature])['target'].mean().sort_values().index\n",
        "        labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n",
        "        data[feature]=data[feature].map(labels_ordered)\n",
        "    \n",
        "    #convert all the values bw 0 and 1\n",
        "    feature_scaling=[feature for feature in data.columns if feature not in ['Id','target']]\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler=MinMaxScaler()\n",
        "    scaler.fit(data[feature_scaling])\n",
        "    data = pd.concat([data[['Id', 'target']].reset_index(drop=True),\n",
        "                      pd.DataFrame(scaler.transform(data[feature_scaling]), columns=feature_scaling)],\n",
        "                      axis=1)\n",
        "    \n",
        "    # 3) Considering sampling ration as 60:40 for only binary classification\n",
        "    if len(data['target'].value_counts())==2:\n",
        "      data=over_under_sampling(data)\n",
        "    \n",
        "    #5 correlation matrix \n",
        "    data=correlation_matrix(data)\n",
        "    #generate descriptive  statistics report \n",
        "\n",
        "    reports=descriptive_statics(data) \n",
        "    print(reports)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}